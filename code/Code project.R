# PROJECT: Predicting High Site Traffic
# Packages
library(tidyverse)
library(readxl)
library(rsample)         
library(marginaleffects)  
library(ranger)          
library(yardstick)  
library(dbplyr)

############################################################

# 1) DATA WRANGLING
# 1.1 Load data (Excel)
traffic_raw <- read_excel("C:/Users/user/Downloads/recipe_site_traffic_2212 .xlsx")

# Inspect data
glimpse(traffic_raw)
summary(traffic_raw)

# 1.2 Recode + type conversion

traffic <- traffic_raw %>%
  mutate(
    high_traffic = case_when(
      high_traffic == "High" ~ 1,
      high_traffic == "NA"   ~ 0,
      is.na(high_traffic)    ~ 0,
      TRUE                   ~ 0
    ),
    category = as.factor(gsub("Chicken Breast", "Chicken", category)),
    
    # parse_number extracts numeric part safely
    servings     = readr::parse_number(as.character(servings)),
    calories     = readr::parse_number(as.character(calories)),
    carbohydrate = readr::parse_number(as.character(carbohydrate)),
    sugar        = readr::parse_number(as.character(sugar)),
    protein      = readr::parse_number(as.character(protein)),
    
    recipe = as.character(recipe))


# 1.3 Check missing values share (like Titanic)
miss_share <- traffic %>%
  summarise(across(everything(), ~ mean(is.na(.x)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "share_missing") %>%
  arrange(desc(share_missing))

miss_share

# 1.4 Drop missing values (course-consistent approach)
traffic_clean <- traffic %>%
  drop_na(calories, carbohydrate, sugar, protein, servings, category, high_traffic)

prop_before <- mean(traffic$high_traffic)          # no NA now because we set NA -> 0
prop_after  <- mean(traffic_clean$high_traffic)

cat("\nProportion High Traffic BEFORE drop_na():", round(prop_before, 4), "\n")
cat("Proportion High Traffic AFTER  drop_na():", round(prop_after, 4), "\n")
cat("Difference (after - before):", round(prop_after - prop_before, 4), "\n")

cat("\nN before:", nrow(traffic), "\n")
cat("N after :", nrow(traffic_clean), "\n")

# The dataset studied contains 947 recipes identified by nutritional characteristics (calories, carbohydrate, sugar, protein), recipe category, number of servings, and an indicator of whether the traffic to the site was high when this recipe was shown.
# We created a response variable "high_traffic" from the same data frame as a subset, originally recorded as "High" for recipes that generate high traffic, and recoded into a binary variable equal to 1 and 0 otherwise ("NA"). We also created two categories (“Chicken” and “Chicken Breast”) for consistency and converted the variable "category" in the data into a factor and nutritional variables and "servings" into numeric after cleaning the data.
# The data had missing values in the nutritional variables that needed to be dropped. To verify that we had the desired outcome, we compared the proportion of high-traffic recipes before and after dropping missing values. The proportion decreased slightly from 0.606 to 0.598, which showed that the outcome distribution was not distorted.


############################################################
# 2) EDA (lesson-style: group means = probabilities)
############################################################

# Make outcome as factor for nicer plots
traffic_clean <- traffic_clean %>%
  mutate(high_traffic_f = factor(high_traffic, levels = c(0, 1), labels = c("Low", "High")))

# 2.1 Nutrient distributions by traffic (density plots)
traffic_clean %>%
  pivot_longer(cols = c(calories, carbohydrate, sugar, protein),
               names_to = "nutrient", values_to = "value") %>%
  ggplot(aes(x = value, fill = high_traffic_f)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ nutrient, scales = "free") +
  theme_bw() +
  labs(title = "Distribution of nutrients by traffic level",
       fill = "Traffic")

# 2.2 Probability of High traffic by category (mean of binary outcome)
traffic_clean %>%
  group_by(category) %>%
  summarise(prob_high = mean(high_traffic), .groups = "drop") %>%
  ggplot(aes(x = reorder(category, prob_high), y = prob_high)) +
  geom_col() +
  coord_flip() +
  theme_bw() +
  labs(title = "Probability of high traffic by category",
       x = "", y = "P(High traffic)")

# Density plots indicate that recipes generating high traffic tend to have higher calorie and protein content, while sugar and carbohydrate distributions overlap substantially across varying traffic levels. This suggests that nutritional richness may matter, but its effect is likely non-linear or secondary relative to other factors.
# Talking about grouping (categories of recipes), we see that high traffic is generated by recipes listed in the categories "Vegetable", "Potato", "Pork", "One Dish Meal", and "Meat" compared to "Beverages", which serves as the baseline category. This means that site visits are more likely to be generated by full-meal recipes than lighter items such as drinks or breakfast recipes.
# Overall, high site traffic is largely explained by recipe category, while nutritional variables may play a more limited role.

############################################################
# 3) TRAIN/TEST SPLIT + (where relevant) 10-fold CV
############################################################

set.seed(123)
split_obj <- initial_split(traffic_clean, prop = 0.7, strata = high_traffic)
train_data <- training(split_obj)
test_data  <- testing(split_obj)

# 10-fold CV object (lesson-style)
set.seed(123)
cv_splits <- vfold_cv(train_data, v = 10, strata = high_traffic)
cv_splits


# The train-test split was divided between a training set (70%) and a test set (30%) by using stratification on the response variable to maintain the proportion of high-traffic recipes among samples. This helps to make sure that the model performance is evaluated using unseen data while conserving class balance.


############################################################
# 4) LPM + LOGIT, Marginal Effects, Variable Importance (|t|)
############################################################

# 4.1 LPM (Linear Probability Model)
lpm_model <- lm(
  high_traffic ~ calories + carbohydrate + sugar + protein + servings + category,
  data = train_data
)
summary(lpm_model)

# Variable importance for LPM = |t-stat|
lpm_t <- summary(lpm_model)$coefficients[-1, 3]
lpm_varimp <- tibble(variable = names(lpm_t), score = abs(lpm_t)) %>%
  arrange(desc(score))

ggplot(lpm_varimp, aes(x = reorder(variable, score), y = score)) +
  geom_col() +
  coord_flip() +
  theme_bw() +
  labs(title = "LPM variable importance (|t-stat|)", x = "", y = "|t|")

# 4.2 Logit (GLM)
logit_model <- glm(
  high_traffic ~ calories + carbohydrate + sugar + protein + servings + category,
  data = train_data,
  family = binomial(link = "logit")
)
summary(logit_model)

# Average Marginal Effects (AME)
ame <- avg_slopes(logit_model)
ame

# Variable importance for Logit = |t-stat| of AME
logit_varimp <- ame %>%
  mutate(score = abs(statistic)) %>%
  arrange(desc(score))

ggplot(logit_varimp, aes(x = reorder(term, score), y = score)) +
  geom_col() +
  coord_flip() +
  theme_bw() +
  labs(title = "Logit variable importance (AME |t-stat|)", x = "", y = "|t|")

# We used OLS to estimate a linear probability model (LPM). As a result, most nutritional variables are not statistically significant. However, the variable "category"  shows a strong and statistically significant effect on high-traffic probability. Also, given the baseline category "Beverages", all main-dish categories are related to higher probabilities of high traffic, showing larger effects for "Vegetable", "Potato", "Pork", and  "one dish meal" recipes.
# Given the binary nature of the response variable, we had to estimate a Logit model and observe the results. It is shown that the signs and significance patterns are equivalent to the LPM model. To allow direct comparison with the LPM coefficients, we computed Average Marginal Effects (AMEs); they confirmed that category effects dominate. For example, for a "vegetable" recipe, the probability of high traffic is increased by more than 90 percentage points relative to Beverages, but in contrast, other nutritional variables are statistically insignificant.
# In order to show the variable importance, we used the absolute value of the t-statistics of the marginal effects. We say that both models show that recipe category is the most important key factor, with nutritional variables playing a less significant role.


############################################
# 5) RANDOM FOREST
############################################

# 5.1 We'll choose mtry that maximizes AUC on validation folds.

# Helper function to compute AUC for one fold
compute_auc_rf <- function(train_df, valid_df, mtry_value) {
  fit <- ranger(
    high_traffic_f ~ calories + carbohydrate + sugar + protein + servings + category,
    data = train_df %>% mutate(high_traffic_f = factor(high_traffic, levels=c(0,1), labels=c("Low","High"))),
    probability = TRUE,
    mtry = mtry_value,
    num.trees = 500,
    importance = "impurity"
  )
  valid_df2 <- valid_df %>%
    mutate(high_traffic_f = factor(high_traffic, levels=c(0,1), labels=c("Low","High")))
  prob <- predict(fit, valid_df2)$predictions[, "High"]
  yardstick::roc_auc(valid_df2, truth = high_traffic_f, prob, event_level = "second")$.estimate
}

# Grid for mtry
library(tidyverse)
library(rsample)
library(ranger)
library(yardstick)
library(marginaleffects)
mtry_grid <- 1:6

set.seed(123)
compute_auc_rf <- function(train_df, valid_df, mtry_value) {
  
  # We wanna ensure outcome is a factor with levels Low/High
  train_df2 <- train_df %>%
    mutate(high_traffic_f = factor(high_traffic, levels = c(0, 1), labels = c("Low", "High")))
  
  valid_df2 <- valid_df %>%
    mutate(high_traffic_f = factor(high_traffic, levels = c(0, 1), labels = c("Low", "High")))
  
  # Fit Random Forest
  fit <- ranger(
    high_traffic_f ~ calories + carbohydrate + sugar + protein + servings + category,
    data = train_df2,
    probability = TRUE,
    num.trees = 500,
    mtry = mtry_value,
    importance = "impurity")
  
  # We get predicted probabilities for "High"
  valid_df2 <- valid_df2 %>%
    mutate(prob_rf = predict(fit, valid_df2)$predictions[, "High"])
  
  # AUC
  yardstick::roc_auc(valid_df2, truth = high_traffic_f, prob_rf, event_level = "second")$.estimate}

cv_results <- map_dfr(mtry_grid, function(m) {
  aucs <- map_dbl(cv_splits$splits, function(spl) {
    tr <- analysis(spl)
    va <- assessment(spl)
    compute_auc_rf(tr, va, m)
  })
  tibble(mtry = m, auc_mean = mean(aucs), auc_sd = sd(aucs))})

cv_results
best_mtry <- cv_results %>% arrange(desc(auc_mean)) %>% slice(1) %>% pull(mtry)
best_mtry

# 5.2 Fit final RF on full training set
train_data <- train_data %>%
  mutate(high_traffic_f = factor(high_traffic, levels=c(0,1), labels=c("Low","High")))
test_data <- test_data %>%
  mutate(high_traffic_f = factor(high_traffic, levels=c(0,1), labels=c("Low","High")))

rf_model <- ranger(
  high_traffic_f ~ calories + carbohydrate + sugar + protein + servings + category,
  data = train_data,
  probability = TRUE,
  num.trees = 500,
  mtry = best_mtry,
  importance = "impurity")

rf_model

# Variable importance
rf_varimp <- tibble(
  variable = names(rf_model$variable.importance),
  importance = as.numeric(rf_model$variable.importance)
) %>% arrange(desc(importance))

ggplot(rf_varimp, aes(x = reorder(variable, importance), y = importance)) +
  geom_col() +
  coord_flip() +
  theme_bw() +
  labs(title = "Random Forest variable importance (impurity)", x = "", y = "Importance")



#################################################
# 6) PREDICTION + CONFUSION MATRICES + ROC/AUC 
#################################################

# 6.1 Predicted probabilities on test set
test_data <- test_data %>%
  mutate(
    prob_lpm   = predict(lpm_model, newdata = test_data),
    prob_logit = predict(logit_model, newdata = test_data, type = "response"))

# RF probabilities
test_data$prob_rf <- predict(rf_model, test_data)$predictions[, "High"]

# 6.2 Class predictions
test_data <- test_data %>%
  mutate(
    pred_lpm   = factor(ifelse(prob_lpm   >= 0.5, "High", "Low"), levels = c("Low","High")),
    pred_logit = factor(ifelse(prob_logit >= 0.5, "High", "Low"), levels = c("Low","High")),
    pred_rf    = factor(ifelse(prob_rf    >= 0.5, "High", "Low"), levels = c("Low","High")))

# 6.3 Confusion matrices
conf_mat(test_data, truth = high_traffic_f, estimate = pred_lpm)
conf_mat(test_data, truth = high_traffic_f, estimate = pred_logit)
conf_mat(test_data, truth = high_traffic_f, estimate = pred_rf)

# 6.4 ROC + AUC
roc_lpm   <- roc_curve(test_data, truth = high_traffic_f, prob_lpm, event_level = "second")
roc_logit <- roc_curve(test_data, truth = high_traffic_f, prob_logit, event_level = "second")
roc_rf    <- roc_curve(test_data, truth = high_traffic_f, prob_rf,  event_level = "second")

auc_lpm   <- roc_auc(test_data, truth = high_traffic_f, prob_lpm, event_level = "second")
auc_logit <- roc_auc(test_data, truth = high_traffic_f, prob_logit, event_level = "second")
auc_rf    <- roc_auc(test_data, truth = high_traffic_f, prob_rf,  event_level = "second")

auc_lpm
auc_logit
auc_rf

# Plot ROC curves together
bind_rows(
  roc_lpm   %>% mutate(model = "LPM"),
  roc_logit %>% mutate(model = "Logit"),
  roc_rf    %>% mutate(model = "Random Forest")
) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_line() +
  geom_abline(linetype = "dashed") +
  theme_bw() +
  labs(title = "ROC curves (test set)", x = "False positive rate", y = "True positive rate")

# The three models almost have the same reasonable performance; as for the LPM anf Logit models, they achieved very equal AUC values (0.808 and 0.809, respectively), and for the Random Forest model, a slightly lower AUC was observed (0.790). Also, confusion matrices were accurate in terms of classification across models, with no significant or clear dominance of the Random Forest.
# We can say that the Logit model is the preferred model, because it achieves the highest AUC on the test set (≈ 0.81), which is slightly greater than the LPM (0.808) and the Random Forest (0.79), also the performance of the LPM and Logit models are nearly identical. Moreover, the Logit model gives better insights for interpreting and understanding through average marginal effects (AME), which is beneficial for actionable decisions. In contrast, despite its flexibility, the Random Forest doesn't help predict performance since it suggests that simpler parametric models largely capture the relationship between predictors and traffic. Given that, the Logit model provides the optimal balance between interpretability and forecast accuracy.
